{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Tweet Classification with LSTM and TF-IDF\n",
    "\n",
    "This notebook classifies tweets related to disasters using Natural Language Processing (NLP) techniques. We will explore the data, clean it, and then use two different models to classify tweets: an LSTM model and a TF-IDF with Random Forest classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "Define key parameters for the models and data processing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 64\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Data\n",
    "Load the training and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "def load_data(train_path='train.csv', test_path='test.csv'):\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = load_data()\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Exploratory Data Analysis (EDA)\n",
    "- View distribution of disaster vs. non-disaster tweets.\n",
    "- Examine tweet lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Target Distribution and Tweet Length Distribution\n",
    "def plot_distributions(train_data):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(x='target', data=train_data)\n",
    "    plt.title(\"Disaster (1) vs. Non-Disaster (0) Tweets Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    train_data['text_length'] = train_data['text'].apply(len)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(train_data['text_length'], bins=30, kde=True)\n",
    "    plt.title(\"Tweet Length Distribution\")\n",
    "    plt.xlabel(\"Tweet Length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "plot_distributions(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Word Cloud Visualization\n",
    "Generate a word cloud for disaster-related tweets to visualize the most common words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word Cloud for Disaster-Related Tweets\n",
    "def plot_wordcloud(text_data):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='black').generate(\" \".join(text_data))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Common Words in Disaster-Related Tweets\")\n",
    "    plt.show()\n",
    "\n",
    "plot_wordcloud(train_data[train_data['target'] == 1]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data Cleaning\n",
    "Remove URLs, punctuation, and stopwords from the tweets to prepare the text for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "def clean_text(text, stopwords):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "def preprocess_data(train_data):\n",
    "    stopwords = set([\"i\", \"me\", \"my\", \"we\", \"our\", \"you\", \"your\", \"he\", \"she\", \"it\", \"they\", \"them\", \n",
    "                     \"what\", \"which\", \"who\", \"this\", \"that\", \"am\", \"is\", \"are\", \"was\", \"were\", \n",
    "                     \"be\", \"been\", \"have\", \"has\", \"do\", \"does\", \"did\", \"a\", \"an\", \"the\", \"and\", \n",
    "                     \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"of\", \"at\", \"by\", \"for\", \"with\", \n",
    "                     \"about\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"to\", \n",
    "                     \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \n",
    "                     \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \n",
    "                     \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"no\", \n",
    "                     \"not\", \"only\", \"same\", \"so\", \"too\", \"very\", \"can\", \"will\", \"just\"])\n",
    "    \n",
    "    train_data['cleaned_text'] = train_data['text'].apply(lambda x: clean_text(x, stopwords))\n",
    "    return train_data\n",
    "\n",
    "train_data = preprocess_data(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Tokenization and Padding\n",
    "Convert text into sequences and pad them to a fixed length for LSTM input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad_sequences(text_data, max_words, max_length):\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(text_data)\n",
    "    sequences = tokenizer.texts_to_sequences(text_data)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "X, tokenizer = tokenize_and_pad_sequences(train_data['cleaned_text'], MAX_WORDS, MAX_SEQUENCE_LENGTH)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, train_data['target'], test_size=TEST_SIZE, random_state=RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Train LSTM Model\n",
    "Use an LSTM model to learn patterns in the sequence data for tweet classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_dim, embedding_dim, input_length):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=input_dim, output_dim=embedding_dim, input_length=input_length),\n",
    "        SpatialDropout1D(0.2),\n",
    "        LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "lstm_model = build_lstm_model(input_dim=MAX_WORDS, embedding_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)\n",
    "history = lstm_model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Evaluate LSTM Model\n",
    "Assess the performance of the LSTM model on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val):\n",
    "    loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f\"Validation Loss: {loss}\")\n",
    "    print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "evaluate_model(lstm_model, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: TF-IDF with Random Forest Classifier\n",
    "Use TF-IDF vectorization with a Random Forest classifier as a comparison to the LSTM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tfidf_rf_model(train_data):\n",
    "    tfidf = TfidfVectorizer(max_features=MAX_WORDS)\n",
    "    X_tfidf = tfidf.fit_transform(train_data['cleaned_text']).toarray()\n",
    "    X_train_tfidf, X_val_tfidf, y_train_tfidf, y_val_tfidf = train_test_split(X_tfidf, train_data['target'], test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "    rf_model.fit(X_train_tfidf, y_train_tfidf)\n",
    "    y_pred_tfidf = rf_model.predict(X_val_tfidf)\n",
    "    \n",
    "    print(\"Random Forest Classifier Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_val_tfidf, y_pred_tfidf))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_val_tfidf, y_pred_tfidf))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_val_tfidf, y_pred_tfidf))\n",
    "\n",
    "train_tfidf_rf_model(train_data)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
